{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **The Application of Deep Reinforcement Learning in Snake Game**\n",
    "\n",
    "Team member: Shujie Cao, Lihui Yi, Jiaqi Zang\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "In recent years, Deep Reinforcement Learning (DRL) has gained significant attention, demonstrating superhuman performances in complex environments such as video games, robot control, etc.\n",
    "\n",
    "In this project, we apply DRL concepts to a classic game, Snake Game. The game of Snake involves controlling a snake in a grid, trying to eat food while avoiding collisions with the snake's own body and the grid borders. The player's score is determined by the amount of food the snake has eaten. The primary goal of our project is to train an AI agent to play this game and maximize the score.\n",
    "\n",
    "We begin by implementing the game of Snake, ensuring we have a well-defined environment with clear state representations, actions, and rewards - critical components of any RL system. We then use Deep Q-Network (DQN) and Double DQN (DDQN) to train our agent, which are both powerful and well-known algorithms that show great performance in many tasks.\n",
    "\n",
    "DQN and DDQN extend traditional Q-Learning by using deep neural networks as function approximators, allowing the agent to handle environments with high-dimensional state spaces. Additionally, in DDQN, in order to reduce the overestimation bias of Q-values, another mechanism is included. It improves the stability and performance of the learning process.\n",
    "\n",
    "Through this project, we not only learned to apply the DRL methods taught in class, but also had the opportunity to face some practical challenges. For example, some way of defining state and action spaces will significantly slow down the learning process, making the task unable to finish in a personal laptop. For details like this, as well as the questions asked in course syllabus, we will have a discussion in Part III.\n",
    "\n",
    "Note that it would be easier to get the idea of our model and big pictures via our video. In this notebook, we mainly talk about the details. Also, since we use the pygame library that Jupyter doesn't support, this Jupyter notebook might not be run successfully. In order to run the code, please copy and paste in a python file. The requirements are up to date packages. Our video link is: https://youtu.be/aTUhJUjlg3I"
   ],
   "metadata": {
    "id": "GhmvoQ2rlmUM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18QvJD4Cb5LG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Part I: Snake Game**\n",
    "\n",
    "\n",
    "---\n",
    "In the following few blocks, we implement this classic snake game by \"pygame\" library. The snake game we wrote follows the same rule as the one we usually see online. We use class attribute in python for implementation. This class encapsulates all the functionality needed to play the game of Snake.\n",
    "\n",
    "> First, we initialize the Snake Game instance. By default, the game UI canvas is a 16 × 12 grid. After initialization, we call function \"reset\" to prepare the game.\n",
    "\n"
   ],
   "metadata": {
    "id": "fFkdzp6Oli9v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a snake game class\n",
    "class SnakeGame:\n",
    "    def __init__(self, width=640, height=480, grid_size=40):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()"
   ],
   "metadata": {
    "id": "dOphDXKJl6nn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "> This method is responsible for resetting or starting a new game. It initializes the Pygame display, sets the game clock, and initializes the snake's starting position and food's position. Without loss of generality, we assume the snake always start around the middle of the grid, while the food is positioned randomly (can even be the same as snake). The output of the function is the initial state at the Genesis time. "
   ],
   "metadata": {
    "id": "PbFOq8yUu26F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to reset/restart the game\n",
    "    def reset(self):\n",
    "        # Initialize pygame\n",
    "        pygame.init()\n",
    "        pygame.display.set_caption('Snake Game')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        self.font = pygame.font.SysFont('Arial', 30)\n",
    "        self.score = 0\n",
    "\n",
    "        # Initialize snake, the initial position is fixed for all games\n",
    "        self.snake = deque()\n",
    "        self.snake.append((self.width / 2, self.height / 2))\n",
    "\n",
    "        # Initialize food\n",
    "        self.food = self.get_random_pos()\n",
    "\n",
    "        # Return the initial state\n",
    "        return self.get_state(0)"
   ],
   "metadata": {
    "id": "6M8CQ145uw9K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "> This method is used to generate a completely random position for the food on the game grid, without caring about whether the food sits on the snake head or snake body."
   ],
   "metadata": {
    "id": "lWblbfdOw_xM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to assign a random position to the food\n",
    "    def get_random_pos(self):\n",
    "        x = random.randint(0, self.width - self.grid_size)\n",
    "        y = random.randint(0, self.height - self.grid_size)\n",
    "        return (x // self.grid_size * self.grid_size, y // self.grid_size * self.grid_size)"
   ],
   "metadata": {
    "id": "DK9huP7VwoOA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "> This function advances the game by one step, given an action input. The action is an integer from {0, 1, 2, 3}, representing the direction the snake moves. First, it updates the position of the snake head. Then, it checks whether the snake hits the borders or itself. If so, the snake dies in this round and a reward of -100 is given.\n",
    "\n",
    " > Afterwards, it checks whether the snake eats the food. If so, a reward of +10 is given. If not, it further checks if the snake head is getting closer to the food. If yes, then a small positive reward of +1 is given. Otherwise, a reward of -1 is given.\n",
    "\n",
    " > The reason we assigns the rewards of -100 and +10 is straightforward, because we want the snake to learn to eat the food and avoid a death. As for the ±1 reward, they are also important and necessary because they encourage the snake to explore the paths and look for the food. We will talk about what happens if these two rewards are removed in Part III.\n",
    "\n",
    " > Different from the reward, if and only if the snake eats the food, we add one score to this game.\n",
    "\n",
    " > At the end of this function, we update the snake body. The way we do that is not moving the entire snake by one step. Instead, we simply add another cell as the new snake head and remove the snake tail. In this way, the code can be simplified a little bit. \n",
    "\n",
    " > Finally, the function returns a tuple containing a boolean indicating whether the game is still running, the current score, and a reward."
   ],
   "metadata": {
    "id": "KEoNtQ1qxhSe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to take a step in the game\n",
    "    def step(self, action):\n",
    "        # Action space: 0 - UP, 1 - RIGHT, 2 - DOWN, 3 - LEFT\n",
    "\n",
    "        # Initialize the reward\n",
    "        reward = 0\n",
    "\n",
    "        # Get the current position of the snake head\n",
    "        x, y = self.snake[0]\n",
    "\n",
    "        # Update the position of the snake head\n",
    "        if action == 0:\n",
    "            y -= self.grid_size\n",
    "        elif action == 1:\n",
    "            x += self.grid_size\n",
    "        elif action == 2:\n",
    "            y += self.grid_size\n",
    "        elif action == 3:\n",
    "            x -= self.grid_size\n",
    "\n",
    "        # Check if the snake is dead, if dead, reward is -200\n",
    "        if x < 0 or x >= self.width or y < 0 or y >= self.height or (x, y) in self.snake:\n",
    "            reward = -100\n",
    "            return False, self.score, reward\n",
    "\n",
    "        # Update the position of the snake\n",
    "        self.snake.appendleft((x, y))\n",
    "\n",
    "        # Check if the snake eats the food\n",
    "        if (x, y) == self.food:\n",
    "            reward = 10\n",
    "            self.score += 1\n",
    "            self.food = self.get_random_pos()\n",
    "            # Check if the new food is on the snake, if so, assign a new position to the food\n",
    "            while self.food in self.snake:\n",
    "                self.food = self.get_random_pos()\n",
    "\n",
    "        # else if the snake gets closer to the food, if so, reward is 1 and remove the snake tail\n",
    "        elif (x - self.food[0]) ** 2 + (y - self.food[1]) ** 2 < (self.snake[1][0] - self.food[0]) ** 2 + (self.snake[1][1] - self.food[1]) ** 2:\n",
    "            reward = 1\n",
    "            self.snake.pop()\n",
    "\n",
    "        # else if the snake gets farther to the food, if so, reward is -1 and remove the snake tail\n",
    "        else:\n",
    "            reward = -1\n",
    "            self.snake.pop()\n",
    "\n",
    "        return True, self.score, reward"
   ],
   "metadata": {
    "id": "DfAzrp4Mxh3w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Part II: Deep reinforcement training**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> In the following blocks, we will give the codes for the training using two models: Deep Q-Network and Double Deep Q-Network. First, we define a DQN policy network with three Hidden Layers.\n"
   ],
   "metadata": {
    "id": "WBtcWuJU7CPI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a DQN agent\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Input Layer of state size 12 and Hidden Layer with 128 nodes\n",
    "    X = Dense(128, input_shape=input_shape, activation=\"relu\", )(X_input)\n",
    "\n",
    "    # Hidden layer with 128 nodes\n",
    "    X = Dense(128, activation=\"relu\")(X)\n",
    "\n",
    "    # Hidden layer with 128 nodes\n",
    "    X = Dense(128, activation=\"relu\")(X)\n",
    "\n",
    "    # Output Layer with # of actions: 4 nodes (up, right, down, left)\n",
    "    X = Dense(action_space, activation=\"softmax\")(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='SnakeGame_DQN_model')\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.00025), metrics=[\"accuracy\"])\n",
    "\n",
    "    # model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "id": "cnNGd6NF7m-0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> We start our DQN agent by setting up some parameters."
   ],
   "metadata": {
    "id": "pwC74u4p882n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a DQN agent\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = SnakeGame()\n",
    "        self.state_size = 13\n",
    "        self.action_size = 4\n",
    "        self.EPISODES = 50\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 500\n",
    "        self.train_start = 500\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space=self.action_size)"
   ],
   "metadata": {
    "id": "6ua6LFoB7sbF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The remember method is responsible for storing the agent's experiences in a memory buffer and gradually reducing the exploration rate as the agent gains more experience and begins training."
   ],
   "metadata": {
    "id": "jV0wPba389v_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to remember the state, the action, the reward, and the next state\n",
    "    def remember(self, state, action, reward, next_state, running):\n",
    "        self.memory.append((state, action, reward, next_state, running))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay"
   ],
   "metadata": {
    "id": "28AEik5P8-TP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The act method implements an exploration-exploitation strategy for the snake agent. It randomly selects actions during the exploration phase, avoiding actions that make the snake go back. During the exploitation phase, it selects the action with the highest predicted Q-value, again avoiding actions that make the snake go back."
   ],
   "metadata": {
    "id": "d_Z15SVr9Bk_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to pick an action given a state, note that the snake is not allowed to go back\n",
    "    def act(self, state):\n",
    "        while True:\n",
    "            # exploration\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                action_back = random.randrange(self.action_size)\n",
    "                if action_back == 0 and state[0][2] == 1:\n",
    "                    continue\n",
    "                elif action_back == 1 and state[0][3] == 1:\n",
    "                    continue\n",
    "                elif action_back == 2 and state[0][0] == 1:\n",
    "                    continue\n",
    "                elif action_back == 3 and state[0][1] == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    return action_back\n",
    "            # exploitation\n",
    "            else:\n",
    "                action_back = np.argmax(self.model.predict(state, verbose=0)[0])\n",
    "                if action_back == 0 and state[0][2] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 0:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "                elif action_back == 1 and state[0][3] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 1:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "                elif action_back == 2 and state[0][0] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 2:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "                elif action_back == 3 and state[0][1] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 3:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "\n",
    "                return action_back"
   ],
   "metadata": {
    "id": "p_cc3MMR9CYM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The replay method implements the experience replay mechanism in the DQN algorithm. It samples a mini-batch of experiences from the agent's memory, computes the target Q-values for each experience, and trains the DQN model using the states and target Q-values from the minibatch. This replay process helps stabilize and improve the learning process of the DQN agent."
   ],
   "metadata": {
    "id": "GW6jCb-99F8G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to replay the memory and train the network\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, running = [], [], []\n",
    "\n",
    "        # assign data into state, next_state, action, reward and running from minibatch\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            running.append(minibatch[i][4])\n",
    "\n",
    "        # Compute the value function of current and next state\n",
    "        target = self.model.predict(state, verbose=0)\n",
    "        target_next = self.model.predict(next_state, verbose=0)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if not running[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches where target is the value function\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)"
   ],
   "metadata": {
    "id": "uojH04r59GTB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    " > The load() method loads a pre-trained DQN model from a file, and the save() method saves the current DQN model to a file. These methods enable the agent to persist the model's learned knowledge and reuse it for testing or further training at a later time without having to train the model from scratch."
   ],
   "metadata": {
    "id": "bIxTBgWM9JsK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to load the model for testing purpose\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    # Function to save the model for testing purpose\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ],
   "metadata": {
    "id": "6xtyotmN9KQ5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The training method executes the training process for the DQN agent on a snake game environment. It iterates over episodes, interacts with the environment, stores experiences in memory, performs model training through replay, and saves the trained model at the end of the training process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "x0I5lTlY9NtB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # Function to train the model\n",
    "    def training(self):\n",
    "        total_episodes = []\n",
    "        total_rewards = []\n",
    "        total_scores = []\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            running = True\n",
    "            i = 0\n",
    "            while running:\n",
    "                action = self.act(state)\n",
    "                running, score, reward, next_state = self.env.run(action)\n",
    "\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "\n",
    "                self.remember(state, action, reward, next_state, running)\n",
    "                state = next_state\n",
    "\n",
    "                i += reward\n",
    "                if not running:\n",
    "                    dateTimeObj = datetime.now()\n",
    "                    timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
    "                    print(\"episode: {}/{}, score: {}, total reward: {}, e: {:.2}, time: {}\".format(e + 1, self.EPISODES, score, i, self.epsilon,\n",
    "                                                                                 timestampStr))\n",
    "                    total_episodes.append(e + 1)\n",
    "                    total_rewards.append(i)\n",
    "                    total_scores.append(score)\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "        # Save the trained model\n",
    "        print(\"Saving trained model as SnakeGame-dqn-training.h5\")\n",
    "        self.save(\"SnakeGame-dqn-training.h5\")\n",
    "\n",
    "        return total_episodes, total_rewards, total_scores"
   ],
   "metadata": {
    "id": "7tTwPBjf9N__"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The test method evaluates the performance of the trained DQN agent on a snake game environment using the saved model. It loads the model, interacts with the environment using the model's predictions to choose actions, and prints the rewards achieved in each episode."
   ],
   "metadata": {
    "id": "MYykKT1W9RcG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    # test function if you want to test the learned model\n",
    "    def test(self):\n",
    "        self.load(\"SnakeGame-dqn-training.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            running = True\n",
    "            i = 0\n",
    "            while running:\n",
    "                action = np.argmax(self.model.predict(state, verbose=0))\n",
    "                running, score, reward, next_state = self.env.run(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += reward\n",
    "\n",
    "                if not running:\n",
    "                    print(\"episode: {}/{}, reward: {}\".format(e + 1, self.EPISODES, i))\n",
    "                    break"
   ],
   "metadata": {
    "id": "JVsLMC3K9SKw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Since DDQN and DQN are similar, some of the codes are the same. Thus, we briefly talk about their similarity and difference rather than explaining the codes function by function like DQN. \n",
    "\n",
    "> First, both DQN and DDQN make use of deep neural networks to estimate the Q-values of state-action pairs, which represent the \"quality\" of taking a particular action in a given state. The difference between them is that how they handle the common issue of overestimation in Q-learning algorithms.\n",
    "\n",
    "> In DQN, the same network is used for both the selection of the action and the generation of the Q-value target during the training. This means that DQN tends to overestimate the Q-values due to the max operator in the Bellman equation, which can lead to instability in training and suboptimal performance.\n",
    "\n",
    "> To address this, DDQN uses a different approach. Instead of using a single network, DDQN utilizes two networks: the online network for action selection and the target network for generating the Q-value target. This small difference can substantially reduce the overestimation bias and leads to improved and more stable performance in many problems. Our numerical experiment results in video also revealed such feature.\n",
    "\n",
    "> Due to page limit and meanwhile keep the completeness of our project, we leave the DDQN codes at the end of this report as an appendix."
   ],
   "metadata": {
    "id": "y-_tAsML9Umi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent_DQN = DQNAgent()\n",
    "    episodes_DQN, rewards_DQN, scores_DQN = agent_DQN.training()\n",
    "\n",
    "    # agent_DDQN = DDQNAgent()\n",
    "    # episodes_DDQN, rewards_DDQN, scores_DDQN = agent_DDQN.training()\n",
    "\n",
    "    agent_DQN.test()"
   ],
   "metadata": {
    "id": "QwwkFkQpRcPr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Part III: Discussion**\n",
    "\n",
    "---\n",
    "In this part, we discuss the performances, model selections and parameters of DQN/DDQN algorithms in the Snake Game by answering the questions asked in the course syllabus. \n",
    "\n",
    "\n",
    "1.   *How do the algorithms perform in terms of training time?*\n",
    "> In the current model settings, both algorithms perform very well in terms of training time. Specifically, it takes only 5min (20 episodes) to train the AI to have a pretty \"reasonable\" action, meaning that the snake begins to not touch the borders and try to go straight to the food. After training for 25min (50 episodes), the snake already knows how to avoid eating itself and meanwhile pick the shortest path to eat the food. After 1 day (1000 episodes) of training, our AI snake can even eat 1/3 of the maximum possible number of food, which our team members cannot achieve by playing manually. \n",
    "\n",
    "2.   *How close are the results to expected results?*\n",
    "> Our expectation of the results is that the AI snake should play better than us. Experiments show that our AI snake indeed has already achieved this expectation. However, in term of the theoretical bound, this snake game is in fact mathematically solvable. It is an instance of the Hamiltonian cycle problem. Therefore, the optimal result is that the snake can grow until it full fills the entire grid. However, it is hard to achieve that theoretical optimal result. Because it needs the player to not be smart but be completely rational. In other words, the snake can always do the following: go right to the right border, go down by one step, go left to the left border, go down by one step... Though this snake can eat the food eventually, it has a very low efficiency and that solution is not \"smart\" or interesting.\n",
    "\n",
    "3.   *How did you adjust the parameters of your algorithm to solve the problem?*\n",
    "4.   *What parameters played an important role in solving the problem?*\n",
    "5.   *What were challenges when working with the algorithms?*\n",
    "> We answer these three questions together. The parameters and model in our problem are carefully designed. We also tried some other thoughts on the parameter and model. Among all those trials, the current setting is the best one. \n",
    "\n",
    " > As a first attempt, we tried to use an ideal definition of state space, which is to let the state be the coordinates of snake head, snake body and food. However, that didn't work well because there are millions of possible states. The learning rate is so slow and it requires a lot of cache memory to train. We found that it is not practical in our laptops. But it will be the best way to design the model if the computational resources are sufficient enough, because it gives complete information of this game to the AI agent. \n",
    "\n",
    " > Our current setting is to only record directions in state space. For example, one state is whether the food is on the right of the snake head. There are only two possible answers: true or false. In this way, we only have 13 state variables. All variables are binary, so total number of possible states are just a few hundreds. As shown in the video, the training process of this model is very fast. \n",
    "\n",
    " > The reward parameters are set to be +10 if the snake eats the food, -100 if the snake dies, +1 if the snake gets closer to the food, -1 if the snake gets farther from the food. The reason we add ±1 rewards is as follows. What happens if we remove these two rewards? Without them, after a long time of training, we found that the snake becomes lazy and conservative eventually. The AI discovers that the chance of eating the food is too small, compared to the chance of being dead. It is simply too risky to chase the food. As a result, the \"smart\" AI decides to travel in a loop. Though it can't gain anything, it won't lose anything either. So in the long-term perspective, traveling in a loop is indeed a good strategy.\n",
    "\n",
    " > Also as shown in the video, we design a state, named \"enclose\", to indicate whether the snake encloses itself. By adding this state, the snake learns to avoid putting itself in an enclosed situation. For details, please see our video for a better demonstration.  \n",
    "\n",
    "6.   *How could you improve your results with future work?*\n",
    " > We think our result of training 1 day (1000 episodes) has been closed to converge. That means, training more time probably won't improve the performance significantly. This is because, the state size in our model is only 13. Basically, the AI snake has limited information about the real situation in game. Therefore, the Q-values learned by the agent must be somehow biased, even if we use the DDQN algorithm to eliminate overestimation. One good way to improve the results is by introducing more information. For example, we can use a Convolutional Neural Network (CNN) and pixels as state space.\n"
   ],
   "metadata": {
    "id": "jakmcI1YRj44"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Appendix: DDQN codes**\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "3mbV8JPFmpH5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a Double DQN agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = SnakeGame()\n",
    "        self.state_size = 13\n",
    "        self.action_size = 4\n",
    "        self.EPISODES = 50\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 500\n",
    "        self.train_start = 500\n",
    "        self.TARGET_UPDATE_FREQUENCY = 100\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space=self.action_size)\n",
    "        self.target_model = OurModel(input_shape=(self.state_size,), action_space=self.action_size)\n",
    "        # Initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "    # Function to update the target model to be same with model, after some time interval\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # Function to remember the state, the action, the reward, and the next state\n",
    "    def remember(self, state, action, reward, next_state, running):\n",
    "        self.memory.append((state, action, reward, next_state, running))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # Function to pick an action given a state, note that the snake is not allowed to go back\n",
    "    def act(self, state):\n",
    "        while True:\n",
    "            # exploration\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                action_back = random.randrange(self.action_size)\n",
    "                if action_back == 0 and state[0][2] == 1:\n",
    "                    continue\n",
    "                elif action_back == 1 and state[0][3] == 1:\n",
    "                    continue\n",
    "                elif action_back == 2 and state[0][0] == 1:\n",
    "                    continue\n",
    "                elif action_back == 3 and state[0][1] == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    return action_back\n",
    "            # exploitation\n",
    "            else:\n",
    "                action_back = np.argmax(self.model.predict(state, verbose=0)[0])\n",
    "                if action_back == 0 and state[0][2] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 0:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "                elif action_back == 1 and state[0][3] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 1:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "                elif action_back == 2 and state[0][0] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 2:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "                elif action_back == 3 and state[0][1] == 1:\n",
    "                    action_back = random.randrange(self.action_size)\n",
    "                    while action_back == 3:\n",
    "                        action_back = random.randrange(self.action_size)\n",
    "\n",
    "                return action_back\n",
    "\n",
    "\n",
    "    # Function to replay the memory and train the network\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, running = [], [], []\n",
    "\n",
    "        # assign data into state, next_state, action, reward and running from minibatch\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            running.append(minibatch[i][4])\n",
    "\n",
    "        # Compute the value function of current state using the primary network\n",
    "        target = self.model.predict(state, verbose=0)\n",
    "\n",
    "        # For the next state, use the policy network to select the best action and the target network to evaluate this action\n",
    "        action_values_next_state = self.model.predict(next_state, verbose=0)\n",
    "        best_action_next_state = np.argmax(action_values_next_state, axis=1)\n",
    "        target_next_model = self.target_model.predict(next_state, verbose=0)\n",
    "        Q_values_next_state = target_next_model[range(self.batch_size), best_action_next_state]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if not running[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * Q_values_next_state[i]\n",
    "\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    # Function to load the model for testing purpose\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    # Function to save the model for testing purpose\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    # Function to train the model\n",
    "    def training(self):\n",
    "        total_episodes = []\n",
    "        total_rewards = []\n",
    "        total_scores = []\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            running = True\n",
    "            i = 0\n",
    "            while running:\n",
    "                action = self.act(state)\n",
    "                running, score, reward, next_state = self.env.run(action)\n",
    "\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "\n",
    "                self.remember(state, action, reward, next_state, running)\n",
    "                state = next_state\n",
    "\n",
    "                i += reward\n",
    "                if not running:\n",
    "                    dateTimeObj = datetime.now()\n",
    "                    timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
    "                    print(\"episode: {}/{}, score: {}, total reward: {}, e: {:.2}, time: {}\".format(e + 1, self.EPISODES, score, i, self.epsilon,\n",
    "                                                                                 timestampStr))\n",
    "\n",
    "                    total_episodes.append(e + 1)\n",
    "                    total_rewards.append(i)\n",
    "                    total_scores.append(score)\n",
    "\n",
    "                self.replay()\n",
    "\n",
    "            # Update target network every TARGET_UPDATE_FREQUENCY episodes\n",
    "            if e % self.TARGET_UPDATE_FREQUENCY == 0:\n",
    "                self.update_target_model()\n",
    "\n",
    "        return total_episodes, total_rewards, total_scores\n",
    "\n",
    "    # test function if you want to test the learned model\n",
    "    def test(self):\n",
    "        self.load(\"SnakeGame-dqn-training.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            running = True\n",
    "            i = 0\n",
    "            while running:\n",
    "                action = np.argmax(self.model.predict(state, verbose=0))\n",
    "                running, score, reward, next_state = self.env.run(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += reward\n",
    "\n",
    "                if not running:\n",
    "                    print(\"episode: {}/{}, reward: {}\".format(e + 1, self.EPISODES, i))\n",
    "                    break"
   ],
   "metadata": {
    "id": "KVNc2mQQmwwb"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
